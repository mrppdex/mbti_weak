{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /Users/aspiela/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aspiela/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "import re\n",
    "from collections import defaultdict, OrderedDict\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfid vectorizer restored...\n",
      "TSVD object restored...\n",
      "Tokenizer restored...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mtbi_augmented_bert_512.csv\")\n",
    "\n",
    "def create_labels(df, source, labels):\n",
    "    # 'INTJ'\n",
    "    new_df_dict = defaultdict(list)\n",
    "    for personality_type in df[source]:\n",
    "      for i, l in enumerate(labels):\n",
    "        new_df_dict[l].append(int(personality_type[i] == l))\n",
    "    return pd.DataFrame(new_df_dict)\n",
    "    #return np.array([x[0] == 'I', x[1] == 'N', x[2] == 'T', x[3] == 'J'], dtype=int)\n",
    "\n",
    "label_df = create_labels(df, 'type', 'INTJ')\n",
    "\n",
    "personality = \"I\"\n",
    "\n",
    "X, y = RandomUnderSampler().fit_resample(df.posts.values[:,None],\n",
    "                                         label_df[personality])\n",
    "\n",
    "train_features, dev_features, train_labels, dev_labels = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "train_features = train_features.flatten()\n",
    "dev_features = dev_features.flatten()\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1500\n",
    "\n",
    "train_df = pd.DataFrame({'posts': train_features,\n",
    "                         'labels': train_labels})\n",
    "\n",
    "eval_df = pd.DataFrame({'posts': dev_features,\n",
    "                        'labels': dev_labels})\n",
    "\n",
    "try:\n",
    "    with open(\"tfid_vectorizer.pkl\", \"rb\") as tf:\n",
    "        tfid_vectorizer = pickle.load(tf)\n",
    "    print(\"Tfid vectorizer restored...\")\n",
    "except:\n",
    "    tfid_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  max_df=0.5,\n",
    "                                  stop_words='english')\n",
    "    tfid_vectorizer.fit(train_features)\n",
    "    with open(\"tfid_vectorizer.pkl\", \"wb\") as tf:\n",
    "        pickle.dump(tfid_vectorizer, tf)\n",
    "    print(\"Tfid vectorizer saved...\")\n",
    "\n",
    "tfid_features = tfid_vectorizer.transform(train_features)\n",
    "\n",
    "try:\n",
    "    with open(\"tsvd_red.pkl\", \"rb\") as f:\n",
    "        tsvd_red = pickle.load(f)\n",
    "    print(\"TSVD object restored...\")\n",
    "except:\n",
    "    tsvd_red = TruncatedSVD(n_components=500)\n",
    "    tsvd_red.fit(tfid_features)\n",
    "    with open(\"tsvd_red.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tsvd_red, f)\n",
    "    print(\"TSVD object saved...\")\n",
    "\n",
    "try:\n",
    "    with open(\"posts_tokenizer.pkl\", \"rb\") as f:\n",
    "        posts_tokenizer = pickle.load(f)\n",
    "    print(\"Tokenizer restored...\")\n",
    "except:\n",
    "    posts_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    posts_tokenizer.fit_on_texts(train_features)\n",
    "    with open(\"posts_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(posts_tokenizer, f)\n",
    "    print(\"Tokenizer saved...\")\n",
    "\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_pipeline(df):\n",
    "\n",
    "    def count_words(x):\n",
    "      return len(x.split())\n",
    "\n",
    "    def element_ratio(x, count):\n",
    "      length = len(x.split())\n",
    "      if length > 0:\n",
    "        return float(count/length)\n",
    "      return 0.\n",
    "\n",
    "    def unique_words_ratio(x):\n",
    "      words = x.split()\n",
    "\n",
    "      if len(words) > 0:\n",
    "        return float(len(set(words))/len(words))\n",
    "      return 0.\n",
    "\n",
    "    def process_emoji(x):\n",
    "      regex = r\":[^\\s]+:\"\n",
    "      count = len(re.findall(regex, x))\n",
    "      return element_ratio(x, count)\n",
    "      #return len(re.findall(regex, x))\n",
    "\n",
    "    def exclamation_mark_count(x):\n",
    "      regex = r\"!\"\n",
    "      count = len(re.findall(regex, x))\n",
    "      return element_ratio(x, count)\n",
    "\n",
    "    def question_mark_count(x):\n",
    "      regex = r\"\\?\"\n",
    "      count = len(re.findall(regex, x))\n",
    "      return element_ratio(x, count)\n",
    "\n",
    "    def capital_letters_count(x):\n",
    "      regex = r\"[^A-Z]\"\n",
    "      count = len(re.sub(regex, \"\", x))\n",
    "      return element_ratio(x, count)\n",
    "\n",
    "    def capital_letters_ratio(x):\n",
    "      regex = r\"[^A-Z]\"\n",
    "      cap = re.sub(regex, \"\", x)\n",
    "      if len(x) > 0:\n",
    "        return float(len(cap)/len(x))\n",
    "      return 0.0\n",
    "\n",
    "    def ellypsis_count(x):\n",
    "      regex = r\"\\.\\.\\.\"\n",
    "      count = len(re.findall(regex, x))\n",
    "      return element_ratio(x, count)\n",
    "\n",
    "    def emoji_faces_count(x):\n",
    "      regex = r\"[;:]+[_-]?[\\)\\(]\"\n",
    "      count = len(re.findall(regex, x))\n",
    "      return element_ratio(x, count)\n",
    "\n",
    "    def capitalized_words_ratio(x):\n",
    "      regex = r\"[A-Z]{2,}\\s\"\n",
    "      count = len(re.findall(regex, x))\n",
    "      return element_ratio(x, count)\n",
    "\n",
    "\n",
    "    def pof_list(x):\n",
    "      pof = nltk.pos_tag(word_tokenize(x))\n",
    "      result = [p for _, p in pof]\n",
    "      return result\n",
    "\n",
    "\n",
    "    df[\"number_of_words\"] = df[\"posts\"].apply(count_words)\n",
    "    df[\"emoji\"] = df[\"posts\"].apply(process_emoji)\n",
    "    df[\"exclamation_mark_count\"] = df[\"posts\"].apply(exclamation_mark_count)\n",
    "    df[\"question_mark_count\"] = df[\"posts\"].apply(question_mark_count)\n",
    "    df[\"capital_letters_count\"] = df[\"posts\"].apply(capital_letters_count)\n",
    "    df[\"ellypsis_count\"] = df[\"posts\"].apply(ellypsis_count)\n",
    "    df[\"emoji_faces_count\"] = df[\"posts\"].apply(emoji_faces_count)\n",
    "    df[\"capitalized_words_ratio\"] = df[\"posts\"].apply(capitalized_words_ratio)\n",
    "    df[\"unique_words_ratio\"] = df[\"posts\"].apply(unique_words_ratio)\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    print(\"**** Parts of Speech Decomposition ****\")\n",
    "\n",
    "    df[\"pof_list\"] = df[\"posts\"].apply(pof_list)\n",
    "\n",
    "    print(\"Time: {:.2f} min\".format((time()-t0)/60))\n",
    "\n",
    "    convtag_dict={'ADJ':['JJ','JJR','JJS'], 'ADP':['EX','TO'], 'ADV':['RB','RBR','RBS','WRB'], 'CONJ':['CC','IN'],'DET':['DT','PDT','WDT'],\n",
    "              'NOUN':['NN','NNS','NNP','NNPS'], 'NUM':['CD'],'PRT':['RP'],'PRON':['PRP','PRP$','WP','WP$'],\n",
    "              'VERB':['MD','VB','VBD','VBG','VBN','VBP','VBZ'],'.':['#','$',\"''\",'(',')',',','.',':'],'X':['FW','LS','UH']}\n",
    "\n",
    "    expanded_convtag = {}\n",
    "    for pos_key, pos_list in convtag_dict.items():\n",
    "      for el in pos_list:\n",
    "        expanded_convtag[el] = pos_key\n",
    "\n",
    "    def count_pos(x):\n",
    "      counter = defaultdict(float)\n",
    "      x_length = len(x)\n",
    "\n",
    "      for w in x:\n",
    "        if w in expanded_convtag.keys():\n",
    "          counter[expanded_convtag[w]] += 1\n",
    "        else:\n",
    "          counter[w] += 1\n",
    "\n",
    "      for k, v in counter.items():\n",
    "        counter[k] = v/x_length\n",
    "\n",
    "      counter_dict = OrderedDict()\n",
    "      for k in sorted(convtag_dict.keys()):\n",
    "          counter_dict[k] = counter.get(k,0.0)\n",
    "\n",
    "      return counter_dict\n",
    "\n",
    "    def count_all_pos(df, column):\n",
    "      pos_df = None\n",
    "      for i in range(len(df)):\n",
    "        if pos_df is None:\n",
    "          pos_df = pd.DataFrame(index=[0], data=count_pos(df.loc[i, column]))\n",
    "        else:\n",
    "          pos_df = pos_df.append(pd.DataFrame(index=[i], data=count_pos(df.loc[i, column])), sort=True)\n",
    "      return pos_df\n",
    "\n",
    "    pos_df = count_all_pos(df, 'pof_list')\n",
    "    pos_df.fillna(0.0, inplace=True)\n",
    "\n",
    "    tfid_features = tfid_vectorizer.transform(df.posts)\n",
    "\n",
    "    tfid_features_dimred = tsvd_red.transform(tfid_features)\n",
    "\n",
    "    dense_features = np.concatenate([df.iloc[:,3:-1].values, pos_df.values, tfid_features_dimred], axis=1)\n",
    "\n",
    "    print('**** Initializing Scaler ***')\n",
    "    try:\n",
    "        with open(\"scaler.pkl\", \"rb\") as sf:\n",
    "            scaler = pickle.load(sf)\n",
    "    except FileNotFoundError:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(dense_features)\n",
    "        with open(\"scaler.pkl\", \"wb\") as sf:\n",
    "            pickle.dump(scaler, sf)\n",
    "\n",
    "    scaled_dense_features = scaler.transform(dense_features)\n",
    "\n",
    "    y = df.labels.values\n",
    "    \n",
    "    return scaled_dense_features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Parts of Speech Decomposition ****\n",
      "Time: 3.45 min\n",
      "**** Initializing Scaler ***\n"
     ]
    }
   ],
   "source": [
    "a, b = preprocess_pipeline(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14179, 520)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14179,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24799817, 0.17736486, 0.35109638, 0.47505374, 0.46806628,\n",
       "       0.29649217, 0.        , 0.51250504, 0.0777027 , 0.83009414,\n",
       "       0.        , 0.32244214])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,21-12:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.pof_list(x)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pof_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.head(1).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
